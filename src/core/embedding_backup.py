# src/core/embedding_backup.py
# ìˆ˜ì • ì „ ì½”ë“œ (layoutlm)ìœ¼ë¡œ ì„ë² ë”© ì¶”ì¶œ
import os
import json
import torch
import warnings
from PIL import Image
from transformers import LayoutLMv3Model, LayoutLMv3Processor

# ê²½ê³  ë©”ì‹œì§€ ë„ê¸°
warnings.filterwarnings("ignore")

# 1. ì„¤ì • (Configuration)
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
MODEL_PATH = os.path.join(BASE_DIR, "models/layoutlmv3_finetuned.pt")
BASE_MODEL_NAME = "microsoft/layoutlmv3-base"

# ë””ë°”ì´ìŠ¤ ì„¤ì •
if torch.cuda.is_available():
    DEVICE = "cuda"
elif torch.backends.mps.is_available():
    DEVICE = "mps"
else:
    DEVICE = "cpu"

# 2. í’€ë§ ì „ëµ êµ¬í˜„
def mean_pooling(last_hidden_state, attention_mask):
    # Mask í™•ì¥ [Batch, Seq] -> [Batch, Seq, 1]
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()
    
    # Sum
    sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, dim=1)
    
    # Count (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)
    sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)
    
    return sum_embeddings / sum_mask

def cls_pooling(last_hidden_state):
    return last_hidden_state[:, 0, :]

# 3. ëª¨ë¸ ë¡œë”©
def get_embedding_model():
    print(f"ğŸ”„ Loading Base Model ({BASE_MODEL_NAME}) on {DEVICE}...")
    processor = LayoutLMv3Processor.from_pretrained(BASE_MODEL_NAME, apply_ocr=False)
    model = LayoutLMv3Model.from_pretrained(BASE_MODEL_NAME)
    
    if os.path.exists(MODEL_PATH):
        if DEVICE == "cpu":
            state_dict = torch.load(MODEL_PATH, map_location="cpu")
        else:
            state_dict = torch.load(MODEL_PATH)
            
        new_state_dict = {}
        for key, value in state_dict.items():
            if "classifier" in key: continue
            if key.startswith("layoutlmv3."):
                new_key = key.replace("layoutlmv3.", "")
                new_state_dict[new_key] = value
            else:
                new_state_dict[key] = value
        
        model.load_state_dict(new_state_dict, strict=False)
        print(f"âœ… Custom weights loaded.")
    else:
        print(f"âš ï¸ Fine-tuned model not found. Using base model.")

    model.to(DEVICE)
    model.eval()
    return model, processor

# 4. ìœ í‹¸ë¦¬í‹°
def normalize_box(box, width, height):
    return [
        int(1000 * (box[0] / width)),
        int(1000 * (box[1] / height)),
        int(1000 * (box[2] / width)),
        int(1000 * (box[3] / height)),
    ]

# 5. ì„ë² ë”© ì¶”ì¶œ (ê°•ë ¥í•œ ë””ë²„ê¹… ë° ì•ˆì „ì¥ì¹˜ ì¶”ê°€)
def extract_embedding(model, processor, image_path, json_path, strategy="mean"):
    try:
        image = Image.open(image_path).convert("RGB")
        width, height = image.size
        
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            
        words = []
        boxes = []
        
        if "lines" in data:
            for line in data["lines"]:
                text = line.get("text", "").strip()
                bbox = line.get("bbox", [])
                if text and len(bbox) == 4:
                    words.append(text)
                    boxes.append(normalize_box(bbox, width, height))
        
        if not words:
            words = [" "]
            boxes = [[0, 0, 0, 0]]

        # 1. í”„ë¡œì„¸ì„œ í˜¸ì¶œ
        encoding = processor(
            image,
            words,
            boxes=boxes,
            return_tensors="pt",
            truncation=True,
            max_length=512
        )
        
        # 2. [ê°•ì œ ë™ê¸°í™”] ê°€ì¥ ì§§ì€ ê¸¸ì´(min_len) ì°¾ê¸°
        input_ids = encoding["input_ids"]
        attention_mask = encoding["attention_mask"]
        bbox = encoding["bbox"]
        
        # í˜„ì¬ ê¸¸ì´ í™•ì¸
        len_ids = input_ids.shape[1]
        len_mask = attention_mask.shape[1]
        len_bbox = bbox.shape[1]
        
        # ë””ë²„ê¹…: ê¸¸ì´ê°€ ë‹¤ë¥´ë©´ ì¶œë ¥
        if len_ids != len_mask or len_ids != len_bbox:
            print(f"âš ï¸ Shape Mismatch Detected! IDs:{len_ids}, Mask:{len_mask}, BBox:{len_bbox}")

        # ê°€ì¥ ì§§ì€ ê¸¸ì´ë¡œ í†µì¼ (ìµœëŒ€ 512)
        min_len = min(len_ids, len_mask, len_bbox, 512)
        
        # 3. ìƒˆë¡œìš´ ì…ë ¥ ë”•ì…”ë„ˆë¦¬ ìƒì„± (Clean Dictionary)
        # ê¸°ì¡´ ë”•ì…”ë„ˆë¦¬ë¥¼ ìˆ˜ì •í•˜ì§€ ì•Šê³ , í™•ì‹¤í•˜ê²Œ ì˜ë¦° ë†ˆë“¤ë§Œ ë‹´ìŠµë‹ˆë‹¤.
        clean_inputs = {
            "input_ids": input_ids[:, :min_len].to(DEVICE),
            "attention_mask": attention_mask[:, :min_len].to(DEVICE),
            "bbox": bbox[:, :min_len, :].to(DEVICE),
            "pixel_values": encoding["pixel_values"].to(DEVICE)
        }

        # 4. ì¶”ë¡ 
        with torch.no_grad():
            outputs = model(**clean_inputs)
            
            # ğŸ” [í•µì‹¬ ìˆ˜ì •] ì¶œë ¥ì—ì„œ í…ìŠ¤íŠ¸ ë¶€ë¶„ë§Œ ë°œë¼ë‚´ê¸°
            # ëª¨ë¸ ì¶œë ¥(692) = í…ìŠ¤íŠ¸(495) + ì´ë¯¸ì§€(197)
            # ìš°ë¦¬ëŠ” í…ìŠ¤íŠ¸ ë§ˆìŠ¤í¬(495)ë¥¼ ì“¸ ê±°ë‹ˆê¹Œ, ì¶œë ¥ë„ ì•ë¶€ë¶„ 495ê°œë§Œ ê°€ì ¸ì™€ì•¼ í•¨.
            
            text_len = clean_inputs["input_ids"].shape[1] # ì˜ˆ: 495
            
            # ì „ì²´ ì¶œë ¥ ì¤‘ ì•ë¶€ë¶„(í…ìŠ¤íŠ¸)ë§Œ ìŠ¬ë¼ì´ì‹±
            text_embeddings = outputs.last_hidden_state[:, :text_len, :] 
            
            if strategy == "mean":
                # ì´ì œ text_embeddings(495)ì™€ attention_mask(495) ê¸¸ì´ê°€ ë”± ë§ìŒ!
                embedding = mean_pooling(text_embeddings, clean_inputs["attention_mask"])
            else: 
                # CLS í† í°ì€ ì–´ì°¨í”¼ 0ë²ˆì§¸ë¼ ìƒê´€ì—†ì—ˆìŒ
                embedding = cls_pooling(text_embeddings)
                
        return embedding[0].cpu().tolist()

    except Exception as e:
        print(f"âŒ Error extracting embedding: {e}")
        return None

# 6. ì‹¤í–‰
if __name__ == "__main__":
    TEST_IMAGE = "data/raw/memo/doc_0047.png" 
    TEST_JSON = "data/processed/ocr/memo/doc_0047.json"
    
    if os.path.exists(TEST_IMAGE) and os.path.exists(TEST_JSON):
        model, processor = get_embedding_model()
        
        print(f"\nğŸ§ª Extracting embedding for: {os.path.basename(TEST_IMAGE)}")
        
        # 1) CLS Pooling
        vec_cls = extract_embedding(model, processor, TEST_IMAGE, TEST_JSON, strategy="cls")
        if vec_cls:
            print(f"ğŸ”¹ [CLS] Vector Dim: {len(vec_cls)}")
            print(f"   Values (First 5): {vec_cls[:5]}")
        
        # 2) Mean Pooling
        vec_mean = extract_embedding(model, processor, TEST_IMAGE, TEST_JSON, strategy="mean")
        if vec_mean:
            print(f"ğŸ”¹ [Mean] Vector Dim: {len(vec_mean)}")
            print(f"   Values (First 5): {vec_mean[:5]}")
            
        print("\nâœ… Verification:")
        if vec_cls and vec_mean and vec_cls != vec_mean:
            print("ğŸ‘‰ Success! Strategies produced different vectors.")
    else:
        print("âŒ Test files not found.")